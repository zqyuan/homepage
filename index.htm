<html>

<head>
<meta http-equiv="Content-Type" content="text/html; charset=gbk">
<script src="https://code.jquery.com/jquery-3.3.1.min.js"></script>
<title>Zhaoquan Yuan (袁召全)'s Homepage </title>
<style>
body {
font-family: "Haas Grot Text R Web", "Helvetica Neue", Helvetica, Arial, sans-serif;
color: #444;
font-size: 62.5%;
background: #f4f4f4;
}

a:link,
a:visited {
color: #1279f8;
text-decoration: none;
}

a:hover {
text-decoration: underline;
}

.main {
display: flex;
}

.left {
font-size: 13px;
line-height: 1.3;
background: #fff;
border-radius: 6px;
-webkit-box-shadow: 0 1px 2px rgba(0, 0, 0, 0.07);
box-shadow: 0 1px 2px rgba(0, 0, 0, 0.07);
position: fixed;
top: 8px;
left: 8px;
bottom: 8px;
float: left;
width: 325px;
min-width: 325px;
margin:0px;
}

.right {
margin: 0 0 0 10px;
flex-grow: 1;
font-size: 14px;
line-height: 1.8;
background: #fff;
border-radius: 6px;
margin-left: 340px;
padding-top:67px;
-webkit-box-shadow: 0 1px 2px rgba(0, 0, 0, 0.07);
box-shadow: 0 1px 2px rgba(0, 0, 0, 0.07);
}

.rightBox {
padding: 5px 100px 20px;
border-top: 1px solid #eee;
position: relative;
}
.rightBox>p{
margin-top: -80px;
padding-top: 80px;
}

.rightBox table {
border-top: 1px solid #e2b0b5;
border-left: 1px solid #e2b0b5;   <!*********************>
}

.rightBox td,
.rightBox th {
border-right: 1px solid #e2b0b5;
border-bottom: 1px solid #e2b0b5;
}

.rightBox th {
padding: 10px;
}

.rightBox td {
font-size: 14px;
line-height: 1.8;
padding: 10px;
}

.right h4 {
font-family: "Roboto Condensed", sans-serif;
margin-bottom: 10px;
position: relative;
color: #000;
font-size: 17px;
line-height: 20px;
}

.text span {
border: orangered 1px solid;
color: #fff;
padding: 0px 8px;
margin-right: 8px;
border-radius: 10px;
color: orangered;
}

.box {
padding: 20px 30px;
border-top: 1px solid #eee;
position: relative;
}

h1 {
margin: 0 0 10px 0;
text-align: center;
font-size: 18px
}

h1 p {
text-align: center;
}

.headerImg {


display: block;
width: 200px;
height: 200px;
margin: 0 auto 5px auto;
border-radius: 10%;
}

h3 {
margin: 0 0 10px 0;
font-size: 13px;
font-weight: 500;
letter-spacing: .02em;
text-transform: uppercase;
color: #999;
}

.wrap {
position: fixed;
top: 8px; right: 8px; left: 348px;
border-bottom: 1px solid #f4f4f4;
background: #fff; z-index: 10;
}
.wrap:before {
display: block;
content: '';
position: fixed;
top: 0px; right: 8px; left: 348px; height: 8px;
background: #f4f4f4;

}

.wrap ul {
padding: 0 30px;
text-align: center;
}

.wrap li {
display: inline-block;
margin: 10px 20px 0 0;
font-size: 16px;
height: 20px;
line-height: 20px;
line-height: 1;
}

.wrap li:hover {}

.wrap li a {
display: inline-block;
color: #444;
}

.wrap li a {
text-decoration: none;
color: #444;
background: #fff;
padding: 6px;
}

.wrap li a:hover {
color: #fff;
background: #3399FF;  <!***************>
padding: 6px;
}
.backtop{
position: fixed;
right: 2rem;
bottom: 2rem;
z-index: 1000;
border: 1px solid red；
}
a>.btnbacktop{
font-size: 2rem;
margin: 1rem 0 0;
padding: 0;
width: 3.33rem;
height: 3.33rem;
line-height: 0;
color: #333;
background-color: #ffffff;
border: 1px solid #e3e8ee;
border-radius: 50%;
box-shadow: 0 0 5px rgba(0,0,0,.05);
}
#hqestop img{opacity:0.6;}

@media screen and (max-width: 1100px) {
.main {
flex-direction: column;

}
.main .wrap {
display: none;
}

.main .left {
width:100%;
left: 0px;
position: relative;
}

.main .right {
width: 100%;
margin: 10px 0 0 0;
box-sizing: border-box;
}
}
</style>
</head>

<!*************************个人信息************************>

<body>
<div class="main">
<div class="left">
<h1>
<p><img alt="" src="https://zqyuan.github.io/homepage/head1003.jpeg" style="WIDTH: 127px; HEIGHT: 180px" /></p>
<p><b>
Zhaoquan Yuan
</b>
</p>
</h1>
<div class="box">

<p> <i>Assistant Professor, Ph.D.<br>
</i></p>



<p><br>
School of Information Science and Technology, Southwest Jiaotong University (SWJTU)</p>
</div>
<div class="box">
<h3>contact</h3>
<p>Office:
Room X9445, No.9 building, School of Information Science and Technology, Southwest Jiaotong University, West Hi-Tech Zone, Chengdu,
China PR.</p>

Email:&nbsp; zqyuan0@gmail.com


<!----------->

</div>
<p></p>
</div>
</div>

<!*************************目录************************>

<div class="right">
<div class="wrap">
<ul>
<li><a href="index.htm">HOMEPAGE</a></li> &nbsp; &nbsp; 
<li><a href="./#RESEARCH">RESEARCH</a></li> &nbsp; &nbsp; 
<li><a href="./#PUBLICATIONS">PUBLICATION</a></li> &nbsp; &nbsp; 
<li><a href="./#RESOURCE">RESOURCE</a></li>
<li><a href="./#recruit">招生信息</a></li>


</ul>

</div>


<!*************************BRIEF BIOGRAPHY************************>

<div class="rightBox text">
<p id="BIOGRAPHY">
<font size="4"><b>BRIEF BIOGRAPHY</b></font>
</p>

Zhaoquan Yuan is an Assistant Professor at the <a href="http://sist.swjtu.edu.cn/index.do?action=index"  target="_blank">School of Information Science and Technology</a>, <a href="https://www.swjtu.edu.cn/"  target="_blank">Southwest Jiaotong University (SWJTU)</a>. 
He graduated with his bachelor's degree from the <a href="http://cs.ustc.edu.cn/"  target="_blank">School of Computer Science and Technology</a>, <a href="https://www.ustc.edu.cn/"  target="_blank">University of Science and Technology of China (USTC)</a>, 
and received his Ph.D. degree in Pattern Recognition and Intelligent System from <a href="http://nlpr-web.ia.ac.cn/mmc/index.html"  target="_blank">Multimedia Computing Group (MMC)</a>, <a href="http://www.nlpr.ia.ac.cn/cn/index.html"  target="_blank">National Laboratory of Pattern Recognition</a>, 
<a href="http://www.ia.cas.cn/"  target="_blank">Institute of Automation, Chinese Academy of Sciences</a>, advised by <a href="http://nlpr-web.ia.ac.cn/mmc/homepage/csxu.html"  target="_blank">Prof. Changsheng Xu</a> (IEEE Fellow, IAPR Fellow, NSFC杰青). He was a research visitor in the China-Singapore Institute of Digital Media (CSIDM) and Department of Computing of The Hong Kong Polytechnic University respectively. 
Also, He was engaged in postdoctoral research in UESTC working with <a href="http://www.lxduan.info/"  target="_blank">Prof. Lixin Duan</a>. 

<br>
<br>

His research interests include multi-modal conversational artificial intelligence, machine learning and reasoning, question answering and dialog, and pattern recognition.

<br>
<br>

</div>

<!*************************************************>


<!*************************** News*************************>
<div class="rightBox">
<p id="NEWS">
<font size="4"><b>NEWS</b></font> &nbsp; 
</p>

<ul>

<li> 
<font size="2" color="red"> [<I>Oct 15, 2020</I>] </italic> </font>  Paper “H. Shao, Z. Yuan, X. Peng, and C. Xu. "Contrastive Learning in Frequency Domain for Non-I.I.D. Image Classification" is accepted by MMM2021.
</li>

<li> 
<font size="2" color="red"> [<I>June 01, 2020</I>] </italic> </font>  Paper “Z. Yuan, S. Sun, L. Duan, C. Li, X. Wu, and C. Xu. "Adversarial Multimodal Network for Movie Question Answering" is accepted by IEEE Transactions on Multimedia.
</li>


</ul>
<br>

</div>



<!****************************** TEACHING******************************>
<div class="rightBox">
<p id="TEACHING">
<font size="4"><b>TEACHING</b></font> &nbsp; 
</p>

<ul>

<li>Digital Image Processing, B1250, 2020--2021(1), Text Book: <a href="https://www.amazon.com/Digital-Image-Processing-Rafael-Gonzalez/dp/0133356728/ref=sr_1_1?keywords=digital+image+processing&qid=1548059657&sr=8-1" target="_blank">Digital Image Processing (4th Edition), Rafael C. Gonzalez</a>.
</li>

<li>Digital Image Processing, B0445, 2019--2020(2), Text Book: <a href="https://www.amazon.com/Digital-Image-Processing-Rafael-Gonzalez/dp/0133356728/ref=sr_1_1?keywords=digital+image+processing&qid=1548059657&sr=8-1" target="_blank">Digital Image Processing (4th Edition), Rafael C. Gonzalez</a>.
</li>

<li>Digital Image Processing, B1932, 2019--2020(1), Text Book: <a href="https://www.amazon.com/Digital-Image-Processing-Rafael-Gonzalez/dp/0133356728/ref=sr_1_1?keywords=digital+image+processing&qid=1548059657&sr=8-1" target="_blank">Digital Image Processing (4th Edition), Rafael C. Gonzalez</a>.
</li>

<li>Digital Image Processing, B0332, 2018--2019(2).
</li>

</ul>

<br>

</div>





<!**********************************RESEARCH*********************************************>
<div class="rightBox">
<p id="RESEARCH">
<font size="4"><b>RECENT RESEARCH</b></font> &nbsp; <a href="#top">Go Top</a>
<br></p>




<!*******************---------------table-------------*************>
<table cellpadding="0" cellspacing="0" width="90%">
<tbody>

<!*******************---------Textbook Question Answering---------*************>
<tr bgcolor="#fff9f9">
<th colspan="2" align="left">
<font color="black"><b>Multi-Modal Machine Comprehension (M3C)</b>

&nbsp;

<!--
[<a href="https://zqyuan.github.io/homepage/">Tutorial</a>]
-->

</font>
</th>
</tr>
<tr>
<td width="200" align="center"><img src="https://zqyuan.github.io/homepage/tqa.png" width="150">
</td>
<td>

Multi-Modal Machine Comprehension (M3C) task builds on the popular Visual Question Answering (VQA) and Machine Comprehension (MC) paradigms by framing question answering as a machine comprehension task, where the context needed to answer questions is provided and composed of both textual languages and visual images. 
We are engaged in developing advanced algorithms for multimodal learning and reasoning, especially for textbook question answering, diagram parsing, multi-step reasoning, neural-symbolic learning, etc.


<br>

<b>Paper/codes: </b>
[<a href="https://zqyuan.github.io/homepage/">Download</a>]
</td>
</tr>
<!*******************---------Textbook Question Answering end---------*************>



<!*****-------------------------Multi-Modal Conversation------------------------****>
<tr bgcolor="#fff9f9">
<th colspan="2" align="left">
<font color="black"><b>Multi-Modal Conversation</b>
&nbsp;


</font>
</th>
</tr>
<tr>
<td width="250" align="center"><img src="https://zqyuan.github.io/homepage/vis-dial.png" width="120">
</td>
<td>

Human-computer dialogue is one of the important research directions of natural language understanding and generation. 
  However, the current human-machine dialogue system is limited to the interactive form of speech or natural language. 
  In recent years, with the popularization and application of voice assistants, virtual digital people, and intelligent service robots, 
  the multi-modal conversation with both "visual and listening" plays a more and more important role in the fields of retail, 
  customer service, media, education, tourism, etc. The multi-modal (text + image) interaction makes the information distribution 
  more efficient and the interaction mode more natural. 
We are engaged in developing advanced algorithms for multi-modal human-computer interaction system to understand the user's 
  intentions in a detailed way and give accurate and fast responses.


<br>

<b>Paper/codes: </b>
[<a href="https://zqyuan.github.io/homepage/">Download</a>]

</td>
</tr>

<!*****-------------------------Multimodal Conversation end------------------------****>



<!--

<!*****-------------------------Graph-based AI------------------------****>
<tr bgcolor="#fff9f9">
<th colspan="2" align="left">
<font color="black"><b>Neural-Symbolic Learning</b>
&nbsp;


</font>
</th>
</tr>
<tr>
<td width="250" align="center"><img src="https://zqyuan.github.io/homepage/vis-dial.png" width="120">
</td>
<td>
In real world, many complicated tasks, such as autonomous driving, public policy decision making, and multi-hop question answering, require understanding the relationship between high-level variables in the data to perform logical reasoning, which is known as System II intelligence. Integrating system I and II intelligence lies in the core of artificial intelligence and machine learning.

Graph is an important structure for System II intelligence, with the universal representation ability to capture the relationship between different variables, and support interpretability, causality, and transferability / inductive generalization. Traditional logic and symbolic reasoning over graphs has relied on methods and tools which are very different from deep learning models, such Prolog language, SMT solvers, constrained optimization and discrete algorithms. Is such a methodology separation between System I and System II intelligence necessary? How to build a flexible, effective and efficient bridge to smoothly connect these two systems, and create higher order artificial intelligence?

Graph neural networks, have emerged as the tool of choice for graph representation learning, which has led to impressive progress in many classification and regression problems such as chemical synthesis, 3D-vision, recommender systems and social network analysis. However, prediction and classification tasks can be very different from logic/symbolic reasoning.


<br>

<b>Paper/codes: </b>
[<a href="https://zqyuan.github.io/homepage/">Download</a>]

</td>
</tr>

<!*****-------------------------Graph-based AI end------------------------****>
-->





<!*******************---------Stable prediction---------*************>
<tr bgcolor="#fff9f9">
<th colspan="2" align="left">
<font color="black"><b>Stable Machine Learning</b>
&nbsp;

<!--
[<a href="https://zqyuan.github.io/homepage/">Tutorial</a>]
-->

</font>
</th>
</tr>
<tr>
<td width="200" align="center"><img src="https://zqyuan.github.io/homepage/stable.png" width="150">
</td>
<td>

Conventional predictive models in machine learning are based on the I.I.D. hypothesis between training and testing data. However, such hypothesis is fragile in real world, and the model minimizing empirical errors on training data does not perform well on testing data, which makes the prediction unstable.
We made efforts to design advanced models to learn invariant representations for stable prediction by combining contrastive learning, causal inference, meta-learning, etc.


<br>

<b>Paper/codes: </b>
[<a href="https://zqyuan.github.io/homepage/">Download</a>]
</td>
</tr>
<!**---------Stable prediction end--------------*>








<!*****-----------------****>

</tbody>
</table>

<br>
<br>

</div>





<!**********************SELECTED PUBLICATIONS**************************>

<div class="rightBox">
<p id="PUBLICATIONS">
<font size="4"><b>SELECTED PUBLICATIONS</b></font>  &nbsp; <a href="#top">Go Top</a>
</p>


<p>
<font size="3">
<b>Journal Papers</b></font>
</p>

<ul>

<li>
  <strong>Zhaoquan Yuan</strong>, Siyuan Sun, Lixin Duan<sup>*</sup>, Changsheng Li<sup>*</sup>, Xiao Wu, Changsheng Xu<br> 
<strong>Adversarial Multimodal Network for Movie Story Question Answering </strong> <br> 
IEEE Transactions on Multimedia (TMM, 中科院JCR一区), doi: 10.1109/TMM.2020.3002667.
</li>
  
<li>Jun-Yan He, Xiao Wu, Zhi-Qi Cheng, <strong>Zhaoquan Yuan</strong>, Yu-Gang Jiang <br> 
<strong>DB-LSTM: Densely-Connected Bi-directional LSTM for Human Action Recognition </strong> <br> 
Neurocomputing (中科院JCR二区), 2020.
</li>
  
<li>
  <strong>Zhaoquan Yuan</strong>, Changsheng Xu<sup>*</sup>, Jitao Sang, Shuicheng Yan, M. Shamim Hossain<br> 
  <strong>Learning Feature Hierarchies: A Layer-Wise Tag-Embedded Approach</strong> <br> 
  IEEE Transactions on Multimedia (TMM, 中科院JCR一区), vol. 17, no. 6, pp. 816-827, June 2015, doi: 10.1109/TMM.2015.2417777.
</li>
  
<li>
<strong>Zhaoquan Yuan</strong>, Jitao Sang, Changsheng Xu<sup>*</sup>, Yan Liu<br> 
  <strong>A Unified Framework of Latent Feature Learning in Social Media </strong> <br> 
  in IEEE Transactions on Multimedia (TMM, 中科院JCR一区), vol. 16, no. 6, pp. 1624-1635, Oct. 2014, doi: 10.1109/TMM.2014.2322338.
</li>

</ul>


<p>
<font size="3">
<b> Conference Papers</b></font>
</p>

<ul>

 <!--
<li>Xiao Peng, <strong>Zhaoquan Yuan<sup>*</sup></strong>,  Huan Shao, Xiao Wu, Changsheng Xu<br> 
 <strong>Meta-Learning Causal Convolutional Network for Stable Prediction </strong><br> 
submitted to the 35th AAAI Conference on Artificial Intelligence (AAAI-21).
</li>
 -->

<li>Huan Shao, <strong>Zhaoquan Yuan<sup>*</sup></strong>, Xiao Peng, Xiao Wu,<br> 
 <strong>Contrastive learning in frequency domain for Non-I.I.D. image classification </strong><br> 
In the 27th International Conference on Multimedia Modeling, 2020 (Accepted).
</li>
  
<li><strong>Zhaoquan Yuan</strong>, Jitao Sang, Yan Liu, Changsheng Xu<sup>*</sup><br> 
 <strong>Latent feature learning in social media network </strong><br> 
In Proceedings of the 21st ACM international conference on Multimedia (ACM MM, CCF-A), pp. 253-262. ACM, 2013.
</li>

<li><strong>Zhaoquan Yuan</strong>, Jitao Sang, Changsheng Xu<sup>*</sup> <br> 
 <strong>Tag-aware image classification via nested deep belief nets </strong><br> 
In 2013 IEEE International Conference on Multimedia and Expo (ICME, CCF-B), pp. 1-6. IEEE, 2013.
</li>

</ul>

<br>
</div>




<!**************** Grant and Funds****************>

<div class="rightBox">
<p id="FUNDS">
<font size="4"><b>GRANT AND FUNDS</b> </font> &nbsp; <a href="#top">Go Top</a>
</p>

<ul>
<li>Machine Reasoning Research Towards Multimodal Question Answering, 2012.01 -- 2022.12, Sichuan Science and Technology Program, 2020YJ0037.</li>

<li>Machine Learning and Reasoning in Video Question Answering, 2019.01 -- 2021.12, the National Natural Science Foundation of China, 61802053.</li>

<li>Semantic Understanding toward Video Question Answering, 2019.01 -- 2020.12, the Fundamental Research Funds for the Central Universities, 2682019CX62.</li>
</ul>
<br>
</div>



<!**************** Services ****************>

<div class="rightBox">
<p id="FUNDS">
<font size="4"><b>PROFESSIONAL ACTIVITIES</b> </font> &nbsp; <a href="#top">Go Top</a>
</p>

<p>
<font size="3">
<b>Program Committee Members</b></font>
</p>

<ul>

<li>ACM International Conference on Multimedia 2020, 2019.</li>
<li>International Conference on Multimedia Modeling 2021, 2020, 2019.</li>
<li>ACM International Conference on Multimedia in Asia 2019.</li>
<li>ACM International Conference on Multimedia Retrieval 2019.</li>
<li>IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2019.</li>
<li>IEEE International Conference on Multimedia and Expo 2019. </li>

</ul>


<p>
<font size="3">
<b>Journal Reviewer</b></font>
</p>

<ul>


<li>IEEE Transactions on Multimedia.</li>
<li>Multimedia Tools and Applications.</li>

<br>
</div>





<!**************** students****************>

<div class="rightBox">
<p id="STUDENTS">
<font size="4"><b>STUDENTS</b> </font> &nbsp; <a href="#top">Go Top</a>
</p>

<p>
<font size="3">
<b>Collaborating Students</b></font>
</p>

<ul>

<li>Huan Shao
</li>

<li>Xiao Peng
</li>

<li>Haitao Fang
</li>

<li>Zhou Du
</li>


</ul>

<br>
</div>





<!*********************** RESOURCE**************************>
<div class="rightBox">
<p id="RESOURCE">
<font size="4"><b>RESOURCE</b></font>  &nbsp; <a href="#top">Go Top</a>
</p>

<p>
<font size="3">
<b>Machine Learning / NLP</b></font>
</p>

<ul>
<li><a href="https://github.com/changwookjun/nlp-paper"  target="_blank">NLP Paper List</a>, 
  <a href="https://github.com/sebastianruder/NLP-progress"  target="_blank">NLP-progress</a>,
  <a href="https://github.com/yizhen20133868/NLP-Conferences-Code"  target="_blank">NLP-Conf-Code</a>.
</li>
  
<li><a href="https://www.dgl.ai/"  target="_blank">Deep Graph Libray</a>
</li>

<li>DRL: <a href="https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"  target="_blank">DRL_Pytorch 1</a>,
  <a href="https://github.com/sweetice/Deep-reinforcement-learning-with-pytorch"  target="_blank">DRL_Pytorch 2</a>
</ul>

  

<p>
<font size="3">
<b>Others</b></font>
</p>

<ul>
<li><a href="http://www.zhuanzhi.ai/topic/2001826911580295" target="_blank">专知知识分享平台</a>
</li>

<li><a href="https://github.com/bharathgs/Awesome-pytorch-list"  target="_blank"> Awesome-Pytorch-list</a>
</li>

</ul>

<br>

</div>




<!*************************招生信息************************>
<div class="rightBox">
<p id="recruit">
<font size="4"><b>招生信息</b></font>  &nbsp; <a href="#top">Go Top</a>
</p>

袁召全，模式识别与智能系统专业博士，中国计算机学会会员，中国人工智能学会会员。本科毕业于中国科学技术大学计算机科学与技术学院，博士毕业于中国科学院自动化研究所 (模式识别国家重点实验室)，师从徐常胜研究员 (IEEE Fellow, 国家杰青, IAPR Fellow)。曾先后在中国-新加坡数字媒体研究院、香港理工大学计算机科学系交流访问。近年来在ACM Multimedia, IEEE Transactions on Multimedia等国际学术会议及期刊发表论文多篇，拥有国家专利2项。任多届MM, ICMR, ICME, MMM, MM Asian等国际会议程序委员会委员，任IEEE Transactions on Multimedia, Multimedia Tools and Applications等国际期刊审稿人。  

主要研究方向包括多模态人工智能、智能问答与对话、机器学习与推理、模式识别等。目前主持国家自然科学基金、四川省科技计划等项目多项。


<br>
<u><I>欢迎有上进心与科研热情、数学与编程基础扎实的同学加入科研团队。有意向的同学请邮件咨询: zqyuan0@gmail.com</I></u>。
<br>
<br>

</div>
<!*************************************************>




<!*****************************updated date***********************************>
<div class="rightBox">

Last updated date: Oct 15, 2020.
<br>
<br>
</div>
<!*********************************Visit tracker**********************************>




</script>

</body>

</html>
