<html>

<head>
<meta http-equiv="Content-Type" content="text/html; charset=gbk">
<script src="https://code.jquery.com/jquery-3.3.1.min.js"></script>
<title>Zhaoquan Yuan 's Homepage </title>
<style>
body {
font-family: "Haas Grot Text R Web", "Helvetica Neue", Helvetica, Arial, sans-serif;
color: #444;
font-size: 62.5%;
background: #f4f4f4;
}

a:link,
a:visited {
color: #1279f8;
text-decoration: none;
}

a:hover {
text-decoration: underline;
}

.main {
display: flex;
}

.left {
font-size: 13px;
line-height: 1.3;
background: #fff;
border-radius: 6px;
-webkit-box-shadow: 0 1px 2px rgba(0, 0, 0, 0.07);
box-shadow: 0 1px 2px rgba(0, 0, 0, 0.07);
position: fixed;
top: 8px;
left: 8px;
bottom: 8px;
float: left;
width: 325px;
min-width: 325px;
margin:0px;
}

.right {
margin: 0 0 0 10px;
flex-grow: 1;
font-size: 14px;
line-height: 1.8;
background: #fff;
border-radius: 6px;
margin-left: 340px;
padding-top:67px;
-webkit-box-shadow: 0 1px 2px rgba(0, 0, 0, 0.07);
box-shadow: 0 1px 2px rgba(0, 0, 0, 0.07);
}

.rightBox {
padding: 5px 40px 20px;
border-top: 1px solid #eee;
position: relative;
}
.rightBox>p{
margin-top: -80px;
padding-top: 80px;
}

.rightBox table {
border-top: 1px solid #0186d2;
border-left: 1px solid #0186d2;   <!*********************>
}

.rightBox td,
.rightBox th {
border-right: 1px solid #0186d2;
border-bottom: 1px solid #0186d2;
}

.rightBox th {
padding: 10px;
}

.rightBox td {
font-size: 14px;
line-height: 1.8;
padding: 10px;
}

.right h4 {
font-family: "Roboto Condensed", sans-serif;
margin-bottom: 10px;
position: relative;
color: #000;
font-size: 17px;
line-height: 20px;
}

.text span {
border: orangered 1px solid;
color: #fff;
padding: 0px 8px;
margin-right: 8px;
border-radius: 10px;
color: orangered;
}

.box {
padding: 20px 30px;
border-top: 1px solid #eee;
position: relative;
}

h1 {
margin: 0 0 10px 0;
text-align: center;
font-size: 18px
}

h1 p {
text-align: center;
}

.headerImg {


display: block;
width: 200px;
height: 200px;
margin: 0 auto 5px auto;
border-radius: 10%;
}

h3 {
margin: 0 0 10px 0;
font-size: 13px;
font-weight: 500;
letter-spacing: .02em;
text-transform: uppercase;
color: #999;
}

.wrap {
position: fixed;
top: 8px; right: 8px; left: 348px;
border-bottom: 1px solid #f4f4f4;
background: #fff; z-index: 10;
}
.wrap:before {
display: block;
content: '';
position: fixed;
top: 0px; right: 8px; left: 348px; height: 8px;
background: #f4f4f4;

}

.wrap ul {
padding: 0 30px;
text-align: center;
}

.wrap li {
display: inline-block;
margin: 10px 20px 0 0;
font-size: 16px;
height: 20px;
line-height: 20px;
line-height: 1;
}

.wrap li:hover {}

.wrap li a {
display: inline-block;
color: #444;
}

.wrap li a {
text-decoration: none;
color: #444;
background: #fff;
padding: 6px;
}

.wrap li a:hover {
color: #fff;
background: #3399FF;  <!***************>
padding: 6px;
}
.backtop{
position: fixed;
right: 2rem;
bottom: 2rem;
z-index: 1000;
border: 1px solid red；
}
a>.btnbacktop{
font-size: 2rem;
margin: 1rem 0 0;
padding: 0;
width: 3.33rem;
height: 3.33rem;
line-height: 0;
color: #333;
background-color: #ffffff;
border: 1px solid #e3e8ee;
border-radius: 50%;
box-shadow: 0 0 5px rgba(0,0,0,.05);
}
#hqestop img{opacity:0.6;}

@media screen and (max-width: 1100px) {
.main {
flex-direction: column;

}
.main .wrap {
display: none;
}

.main .left {
width:100%;
left: 0px;
position: relative;
}

.main .right {
width: 100%;
margin: 10px 0 0 0;
box-sizing: border-box;
}
}
</style>
</head>

<!*************************个人信息************************>

<body>
<div class="main">
<div class="left">
<h1>
<p><img alt="" src="/photo.jpg" style="WIDTH: 200px; HEIGHT: 191px" /></p>
<p><b>
Zhaoquan Yuan (袁召全)
</b>
</p>
</h1>
<div class="box">

<p> <i>Assistant Professor, Ph.D.<br>
</i></p>



<p><br>
School of Information Science and Technology, Southwest Jiaotong University (SWJTU)</p>
</div>
<div class="box">
<h3>contact</h3>
<p>Office:
Room X9441, No.9 building, School of Information Science and Technology, Southwest Jiaotong University, West Hi-Tech Zone, Chengdu,
China PR.</p>

Email:&nbsp; zqyuan0@gmail.com


<!----------->

</div>
<p></p>
</div>
</div>

<!*************************目录************************>

<div class="right">
<div class="wrap">
<ul>
<li><a href="index.htm">HOMEPAGE</a></li> &nbsp; &nbsp; 
<li><a href="./#RESEARCH">RESEARCH</a></li> &nbsp; &nbsp; 
<li><a href="./#PUBLICATIONS">PUBLICATION</a></li> &nbsp; &nbsp; 
<li><a href="./#RESOURCE">RESOURCE</a></li>


</ul>

</div>


<!*************************BRIEF BIOGRAPHY************************>

<div class="rightBox text">
<p id="BIOGRAPHY">
<font size="4"><b>BRIEF BIOGRAPHY</b></font>
</p>


I am an Assistant Professor at the <a href="http://sist.swjtu.edu.cn/index.do?action=index"  target="_blank">School of Information Science and Technology</a>, <a href="https://www.swjtu.edu.cn/"  target="_blank">Southwest Jiaotong University (SWJTU)</a>. 

I graduated with my bachelor's degree from the <a href="http://cs.ustc.edu.cn/"  target="_blank">School of Computer Science and Technology</a>, <a href="https://www.ustc.edu.cn/"  target="_blank">University of Science and Technology of China (USTC)</a>, 
and received my Ph.D. degree in Pattern Recognition and Intelligent System from <a href="http://nlpr-web.ia.ac.cn/mmc/index.html"  target="_blank">Multimedia Computing Group (MMC)</a>, <a href="http://www.nlpr.ia.ac.cn/CN/model/index.shtml"  target="_blank">National Laboratory of Pattern Recognition</a>, 

<a href="http://www.ia.cas.cn/"  target="_blank">Institute of Automation, Chinese Academy of Sciences</a>, advised by <a href="http://nlpr-web.ia.ac.cn/mmc/homepage/csxu.html"  target="_blank">Prof. Changsheng Xu (IEEE Fellow, IAPR Fellow)</a>. I was a research visitor in the China-Singapore Institute of Digital Media (CSIDM) and Department of Computing of The Hong Kong Polytechnic University respectively. 

Also, I was a postdoc researcher in UESTC collaborating with <a href="http://www.lxduan.info/"  target="_blank">Prof. Lixin Duan</a>. 

<br>
<br>

My research interests include multimodal dialogue system, machine learning, and multimedia semantic comprehension.

<br>
<br>

</div>

<!*************************************************>


<!*************************** News*************************>
<div class="rightBox">
<p id="NEWS">
<font size="4"><b>NEWS</b></font> &nbsp; 
</p>

<ul>

<li>I am elected as a member of the Academic Committee (AC) of the CCF Young Computer Scientists & Engineers Forum (CCF YOCSEF), Jan 5, 2019.
</li>


</ul>
<br>

</div>



<!****************************** TEACHING******************************>
<div class="rightBox">
<p id="TEACHING">
<font size="4"><b>TEACHING</b></font> &nbsp; 
</p>

<ul>

<li>Digital Image Processing, B0445, 2019--2020(2), Text Book: <a href="https://www.amazon.com/Digital-Image-Processing-Rafael-Gonzalez/dp/0133356728/ref=sr_1_1?keywords=digital+image+processing&qid=1548059657&sr=8-1" target="_blank">Digital Image Processing (4th Edition), Rafael C. Gonzalez</a>.
</li>

<li>Digital Image Processing, B1932, 2019--2020(1), Text Book: <a href="https://www.amazon.com/Digital-Image-Processing-Rafael-Gonzalez/dp/0133356728/ref=sr_1_1?keywords=digital+image+processing&qid=1548059657&sr=8-1" target="_blank">Digital Image Processing (4th Edition), Rafael C. Gonzalez</a>.
</li>

<li>Digital Image Processing, B0332, 2018--2019(2).
</li>

</ul>

<br>

</div>





<!**********************************RECENT RESEARCH*********************************************>
<div class="rightBox">
<p id="RESEARCH">
<font size="4"><b>RECENT RESEARCH</b></font> &nbsp; <a href="#top">Go Top</a>
<br></p>




<!*******************---------------table-------------*************>
<table cellpadding="0" cellspacing="0" width="95%">
<tbody>

<!*******************---------the first work---------*************>
<tr bgcolor="#CCE5FF">
<th colspan="2" align="left">
<font color="black"><b>Movie Story Question Answering</b>
&nbsp;
[<a href="http://userweb.swjtu.edu.cn/Userweb/zqyuan/index.htm">Tutorial</a>]
</font>
</th>
</tr>
<tr>
<td width="300" align="center"><img src="images/movieqa_light.png" width="280">
</td>
<td>

Visual question answering by using information from multiple modalities has attracted more and more attention in re- cent years. However, it is a very challenging task, as the visual content and natural language have quite different statistical properties. In this work, we present a method called Adversarial Multimodal Network (AMN) to better understand video stories for question answering. In AMN, we propose to learn multimodal feature representations by finding a more coherent subspace for video clips and the corresponding texts (e.g., subtitles and questions) based on generative adversarial networks. Moreover, a self-attention mechanism is developed to enforce our newly introduced consistency constraint in order to preserve the self-correlation between the visual cues of the original video clips in the learned multimodal representations. Extensive experiments on the benchmark MovieQA and TVQA datasets show the effectiveness of our proposed AMN over other published state-of-the-art methods.

<br>
<br>



<b>Datasets and codes: </b>
[<a href="http://movieqa.cs.toronto.edu/home/" target="_blank">MovieQA</a>]&nbsp;
[<a href="http://userweb.swjtu.edu.cn/Userweb/zqyuan/index.htm">Code</a>]
</td>
</tr>

<!--

<!*****-------------------------Motivation Prediction------------------------****>
<tr bgcolor="#CCE5FF">
<th colspan="2" align="left">
<font color="black"><b>Motivation Prediction</b>
&nbsp;
[<a href="http://userweb.swjtu.edu.cn/Userweb/zqyuan/index.htm">Tutorial</a>]
</font>
</th>
</tr>
<tr>
<td width="300" align="center"><img src="images/motivation.jpg" width="280">
</td>
<td>

Understanding potential motivations behind people's actions in images is a key research topic in the computer vision and pattern recognition. It is a very challenging, because motivations are usually beyond plain image pixels and hard to be described visually. To solve this task, we employ high-level image-specific textual information and explore a potential causal structure among the concepts of scenes, actions and motivations. Unlike most existing visual recognition models, PLCR infers the motivations by executing perception learning and causal reasoning seamlessly. 
<br>
<br>


<b>Datasets and codes: </b>
[<a href="http://userweb.swjtu.edu.cn/Userweb/zqyuan/index.htm">Dataset</a>]&nbsp;
[<a href="http://userweb.swjtu.edu.cn/Userweb/zqyuan/index.htm">Code</a>]

</td>
</tr>

-->

</tbody>
</table>


<br>
<br>

</div>


<!**********************SELECTED PUBLICATIONS**************************>

<div class="rightBox">
<p id="PUBLICATIONS">
<font size="4"><b>SELECTED PUBLICATIONS</b></font>  &nbsp; <a href="#top">Go Top</a>
</p>


<p>
<font size="3">
<b>Multimodal QA / Machine Learning</b></font>
</p>

<ul>

<li><strong>Zhaoquan Yuan</strong>, Siyuan Sun, Lixin Duan<sup>*</sup>, Changsheng Li, Xiao Wu,  and Changsheng Xu. 
"Adversarial Multimodal Network for Movie Question Answering."
<a href="https://arxiv.org/abs/1906.09844"  target="_blank">arXiv:1906.09844</a>.
</li>

</ul>



<p>
<font size="3">
<b> Multimedia / Computer vision</b></font>
</p>

<ul>

<!--
<li><strong>Zhaoquan Yuan</strong>, Siyuan Sun, Lixin Duan<sup>*</sup>, Changsheng Li, Xiao Wu,  and Changsheng Xu. 
"Adversarial Multimodal Network for Movie Question Answering."
<a href="https://arxiv.org/abs/1906.09844"  target="_blank">arXiv:1906.09844</a>.
</li>
-->

<li><strong>Zhaoquan Yuan</strong>, Changsheng Xu<sup>*</sup>, and Jitao Sang. 
"Learning Feature Hierarchies: A Layer-wise Tag-embedded Approach." 
IEEE Transactions on Multimedia (TMM, JCR-1区), 2015, 17(6): 816 - 827.
</li>



<li><strong>Zhaoquan Yuan</strong>, Jitao Sang, Changsheng Xu<sup>*</sup>, and Yan Liu. 
"A unified latent feature learning framework in social media." 
IEEE Transactions on Multimedia (TMM, JCR-1区), 2014, 16(6): 1624-1635.
</li>


<li><strong>Zhaoquan Yuan</strong>, Jitao Sang, Yan Liu, and Changsheng Xu<sup>*</sup>.
"Latent feature learning in social media network".
In Proceedings of the 21st ACM international conference on Multimedia (ACM MM, CCF-A), pp. 253-262. ACM, 2013.
</li>


<li><strong>Zhaoquan Yuan</strong>, Jitao Sang, and Changsheng Xu<sup>*</sup>. 
"Tag-aware image classification via nested deep belief nets." 
In 2013 IEEE International Conference on Multimedia and Expo (ICME, CCF-B), pp. 1-6. IEEE, 2013.
</li>

</ul>

<br>

</div>




<!**************** Grant and Funds****************>

<div class="rightBox">
<p id="FUNDS">
<font size="4"><b>GRANT AND FUNDS</b> </font> &nbsp; <a href="#top">Go Top</a>
</p>

<ul>
<li>Machine Learning and Reasoning in Video Question Answering, 2019.01 -- 2021.12, National Natural Science Foundation of China (NSFC).</li>

<li>Semantic Understanding toward Video Question Answering, 2019.01 -- 2020.12, Fundamental Research Funds for the Central Universities.</li>
</ul>


<!--
<li>面向领域大数据的事件知识图谱构建研究，国家自然科学基金 (重点项目)，参研</li>
<li>面向军事情报的多媒体大数据分析与展示，国家自然科学基金 (联合基金项目)，主研</li>
<li>基于深度学习框架的社交媒体信息挖掘，国家自然科学基金 (面上项目)，主研</li>
</ul>
-->

<br>
</div>



<!**************** Services ****************>

<div class="rightBox">
<p id="FUNDS">
<font size="4"><b>SERVICES</b> </font> &nbsp; <a href="#top">Go Top</a>
</p>

<p>
<font size="3">
<b>Program Committee Members</b></font>
</p>

<ul>

<li>International Conference on Multimedia Modeling 2020.</li>
<li>ACM International Conference on Multimedia in Asia 2019.</li>
<li>ACM International Conference on Multimedia 2019.</li>
<li>ACM International Conference on Multimedia Retrieval 2019.</li>
<li>IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2019.</li>
<li>IEEE International Conference on Multimedia and Expo 2019. </li>

</ul>


<p>
<font size="3">
<b>Journal Reviewer</b></font>
</p>

<ul>

<li>Multimedia Tools and Applications.</li>
<li>IEEE Transactions on Multimedia.</li>



<br>
</div>




<!--
<!**************** students****************>

<div class="rightBox">
<p id="STUDENTS">
<font size="4"><b>STUDENTS</b> </font> &nbsp; <a href="#top">Go Top</a>
</p>

<p>
<font size="3">
<b>Students Collaborated With Me</b></font>
</p>

<ul>

<li>Siyuan Sun (UESTC)
</li>



</ul>

<br>
</div>

-->

<!*********************** RESOURCE**************************>

<div class="rightBox">
<p id="RESOURCE">
<font size="4"><b>RESOURCE</b></font>  &nbsp; <a href="#top">Go Top</a>
</p>



<p>
<font size="3">
<b>Machine Learning</b></font>
</p>



<ul>

<li>Generative Adversarial Network: <a href="https://github.com/eriklindernoren/PyTorch-GAN"  target="_blank">PyTorch-GAN</a>, 
<a href="https://github.com/GANs-in-Action/gans-in-action"  target="_blank">GANs in Action</a>.
</li>

<!--
<li>Graph Neural Networks Papers: <a href="https://github.com/nnzhan/Awesome-Graph-Neural-Networks"  target="_blank"> 1</a>, <a href="https://github.com/thunlp/GNNPapers"  target="_blank"> 2</a> ; 
PyTorch Geometric: <a href="https://github.com/rusty1s/pytorch_geometric"  target="_blank">GitHub</a>.
</li>
-->

<!--
<li>PyTorch Geometric: <a href="https://github.com/rusty1s/pytorch_geometric"  target="_blank">GitHub</a>.
</li>
-->

<!--
<li>Awesome Reinforcement Learning: <a href="https://github.com/jgvictores/awesome-deep-reinforcement-learning" target="_blank">1</a>, <a href="https://github.com/tigerneil/awesome-deep-rl" target="_blank">2</a>, <a href="https://github.com/aikorea/awesome-rl" target="_blank">3</a>, <a href="https://www.youtube.com/playlist?list=PLkFD6_40KJIxJMR-j5A1mkxK26gh_qg37" target="_blank">4</a>.
</li>
-->

<li>Deep Reinforcement Learning: <a href="https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"  target="_blank">DRL-with-PyTorch</a>.
<!--
<a href="https://www.youtube.com/playlist?list=PLkFD6_40KJIxJMR-j5A1mkxK26gh_qg37"  target="_blank">CS294-112</a>
</li>
-->



<li> <a href="https://github.com/sudharsan13296/Awesome-Meta-Learning"  target="_blank">Awesome-Meta-Learning</a>.
</li>


</ul>



<!--

<!*****---------------- *Question Answering---------------- ****>
<p>
<font size="3">
<b>Question Answering</b></font>
</p>

<ul>

<li> <a href="https://github.com/seriousran/awesome-qa"  target="_blank">Awesome Question Answering</a>.
</li>


<li> <a href="https://github.com/sebastianruder/NLP-progress"  target="_blank">NLP-progress (QA)</a>.
</li>



</ul>

-->



<!*****---------------- *Vision and Natural Language Processing---------------- ****>
<!--
<p>
<font size="3">
<b>Natural Language Processing  /  Vision </b></font>
</p>

<ul>

<li>Natural Language Processing: <a href="https://www.bilibili.com/video/av29608234"  target="_blank"> Michael Collins (Columbia U)</a>, 
<a href="https://www.bilibili.com/video/av20050023"  target="_blank"> Dan Jurafsky (Stanford)</a>.
</li>


<li> Deep Learning for NLP:  CS11-747 (Graham Neubig, <a href="http://phontron.com/class/nn4nlp2019/"  target="_blank"> Homepage</a>, <a href="https://www.youtube.com/playlist?list=PL8PYTP1V4I8Ajj7sY6sdtmjgkt7eo2VMs"  target="_blank">Video</a>), 
CS224n (Christopher Manning, <a href="http://web.stanford.edu/class/cs224n/index.html"  target="_blank"> Homepage</a>, <a href="https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z"  target="_blank"> Video </a>).
</li>


<li>CNN for Visual Recognition: <a href="http://cs231n.stanford.edu/"  target="_blank">Fei-Fei Li (Stanford)</a>.
</li>


</ul>
-->







<!*****---------------- Causality---------------- *****>
<!--
<p>
<font size="3">
<b>Causality</b></font>
</p>

<ul>
<li><a href="https://github.com/logangraham/arXausality" target="_blank">arXausality.</a>
</li>

<li><a href="https://github.com/Microsoft/dowhy" target="_blank">dowhy.</a>
</li>


<li><a href="https://github.com/akelleh/causality" target="_blank">Tools for causal analysis.</a>
</li>


</ul>
-->




<!*****---------------- Others---------------- ***>

<p>
<font size="3">
<b>Others</b></font>
</p>

<ul>

<li><a href="http://www.zhuanzhi.ai/topic/2001826911580295" target="_blank">专知知识分享平台.</a>
</li>

<li><a href="https://github.com/bharathgs/Awesome-pytorch-list"  target="_blank"> Awesome-Pytorch-list.</a>
</li>

<li>XAI: <a href="https://github.com/h2oai/mli-resources"  target="_blank"> Machine Learning Interpretability</a>, <a href="https://github.com/jphall663/awesome-machine-learning-interpretability#comprehensive-software-examples-and-tutorials"  target="_blank">awesome-machine-learning-interpretability.
</li>



<!--
<li><a href="https://github.com/zqyuan/xai_resources"  target="_blank"> Interesting Resources Related to XAI (Explainable Artificial Intelligence).</a>
</li>

<li><a href="https://github.com/ddbourgin/numpy-ml"  target="_blank"> Numpy-ml.</a>
</li>

<li><a href="https://github.com/brendenlake/AAI-site"  target="_blank"> Advancing AI Through Cognitive Science.</a>
</li>



-->

</ul>
<br>
</div>






<!*****************************updated date***********************************>
<div class="rightBox">


Last updated date: January 16, 2019
<br>
<br>

<!*********************************Visit tracker**********************************>





</script>

</body>

</html>
