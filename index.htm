<html>

<head>
<meta http-equiv="Content-Type" content="text/html; charset=gbk">
<script src="https://code.jquery.com/jquery-3.3.1.min.js"></script>
<title>Zhaoquan Yuan (袁召全) 's Homepage </title>
<style>
body {
font-family: "Haas Grot Text R Web", "Helvetica Neue", Helvetica, Arial, sans-serif;
color: #444;
font-size: 62.5%;
background: #f4f4f4;
}

a:link,
a:visited {
color: #1279f8;
text-decoration: none;
}

a:hover {
text-decoration: underline;
}

.main {
display: flex;
}

.left {
font-size: 13px;
line-height: 1.3;
background: #fff;
border-radius: 6px;
-webkit-box-shadow: 0 1px 2px rgba(0, 0, 0, 0.07);
box-shadow: 0 1px 2px rgba(0, 0, 0, 0.07);
position: fixed;
top: 8px;
left: 8px;
bottom: 8px;
float: left;
width: 325px;
min-width: 325px;
margin:0px;
}

.right {
margin: 0 0 0 10px;
flex-grow: 1;
font-size: 14px;
line-height: 1.8;
background: #fff;
border-radius: 6px;
margin-left: 340px;
padding-top:67px;
-webkit-box-shadow: 0 1px 2px rgba(0, 0, 0, 0.07);
box-shadow: 0 1px 2px rgba(0, 0, 0, 0.07);
}

.rightBox {
padding: 5px 40px 20px;
border-top: 1px solid #eee;
position: relative;
}
.rightBox>p{
margin-top: -80px;
padding-top: 80px;
}

.rightBox table {
border-top: 1px solid #0186d2;
border-left: 1px solid #0186d2;   <!*********************>
}

.rightBox td,
.rightBox th {
border-right: 1px solid #0186d2;
border-bottom: 1px solid #0186d2;
}

.rightBox th {
padding: 10px;
}

.rightBox td {
font-size: 14px;
line-height: 1.8;
padding: 10px;
}

.right h4 {
font-family: "Roboto Condensed", sans-serif;
margin-bottom: 10px;
position: relative;
color: #000;
font-size: 17px;
line-height: 20px;
}

.text span {
border: orangered 1px solid;
color: #fff;
padding: 0px 8px;
margin-right: 8px;
border-radius: 10px;
color: orangered;
}

.box {
padding: 20px 30px;
border-top: 1px solid #eee;
position: relative;
}

h1 {
margin: 0 0 10px 0;
text-align: center;
font-size: 18px
}

h1 p {
text-align: center;
}

.headerImg {


display: block;
width: 200px;
height: 200px;
margin: 0 auto 5px auto;
border-radius: 10%;
}

h3 {
margin: 0 0 10px 0;
font-size: 13px;
font-weight: 500;
letter-spacing: .02em;
text-transform: uppercase;
color: #999;
}

.wrap {
position: fixed;
top: 8px; right: 8px; left: 348px;
border-bottom: 1px solid #f4f4f4;
background: #fff; z-index: 10;
}
.wrap:before {
display: block;
content: '';
position: fixed;
top: 0px; right: 8px; left: 348px; height: 8px;
background: #f4f4f4;

}

.wrap ul {
padding: 0 30px;
text-align: center;
}

.wrap li {
display: inline-block;
margin: 10px 20px 0 0;
font-size: 16px;
height: 20px;
line-height: 20px;
line-height: 1;
}

.wrap li:hover {}

.wrap li a {
display: inline-block;
color: #444;
}

.wrap li a {
text-decoration: none;
color: #444;
background: #fff;
padding: 6px;
}

.wrap li a:hover {
color: #fff;
background: #3399FF;  <!***************>
padding: 6px;
}
.backtop{
position: fixed;
right: 2rem;
bottom: 2rem;
z-index: 1000;
border: 1px solid red；
}
a>.btnbacktop{
font-size: 2rem;
margin: 1rem 0 0;
padding: 0;
width: 3.33rem;
height: 3.33rem;
line-height: 0;
color: #333;
background-color: #ffffff;
border: 1px solid #e3e8ee;
border-radius: 50%;
box-shadow: 0 0 5px rgba(0,0,0,.05);
}
#hqestop img{opacity:0.6;}

@media screen and (max-width: 1100px) {
.main {
flex-direction: column;

}
.main .wrap {
display: none;
}

.main .left {
width:100%;
left: 0px;
position: relative;
}

.main .right {
width: 100%;
margin: 10px 0 0 0;
box-sizing: border-box;
}
}
</style>
</head>

<!*************************个人信息************************>

<body>
<div class="main">
<div class="left">
<h1>
<p><img alt="" src="https://zqyuan.github.io/homepage/profile_photo.png" style="WIDTH: 150px; HEIGHT: 200px" /></p>
<p><b>
Zhaoquan Yuan (袁召全)
</b>
</p>
</h1>
<div class="box">

<p> <i>Assistant Professor, Ph.D.<br>
</i></p>



<p><br>
School of Information Science and Technology, Southwest Jiaotong University (SWJTU)</p>
</div>
<div class="box">
<h3>contact</h3>
<p>Office:
Room X9445, No.9 building, School of Information Science and Technology, Southwest Jiaotong University, West Hi-Tech Zone, Chengdu,
China PR.</p>

Email:&nbsp; zqyuan0@gmail.com


<!----------->

</div>
<p></p>
</div>
</div>

<!*************************目录************************>

<div class="right">
<div class="wrap">
<ul>
<li><a href="index.htm">HOMEPAGE</a></li> &nbsp; &nbsp; 
<li><a href="./#RESEARCH">RESEARCH</a></li> &nbsp; &nbsp; 
<li><a href="./#PUBLICATIONS">PUBLICATION</a></li> &nbsp; &nbsp; 
<li><a href="./#RESOURCE">RESOURCE</a></li>
<li><a href="./#recruit">招生信息</a></li>


</ul>

</div>


<!*************************BRIEF BIOGRAPHY************************>

<div class="rightBox text">
<p id="BIOGRAPHY">
<font size="4"><b>BRIEF BIOGRAPHY</b></font>
</p>

Zhaoquan Yuan is an Assistant Professor at the <a href="http://sist.swjtu.edu.cn/index.do?action=index"  target="_blank">School of Information Science and Technology</a>, <a href="https://www.swjtu.edu.cn/"  target="_blank">Southwest Jiaotong University (SWJTU)</a>. 
He graduated with his bachelor's degree from the <a href="http://cs.ustc.edu.cn/"  target="_blank">School of Computer Science and Technology</a>, <a href="https://www.ustc.edu.cn/"  target="_blank">University of Science and Technology of China (USTC)</a>, 
and received his Ph.D. degree in Pattern Recognition and Intelligent System from <a href="http://nlpr-web.ia.ac.cn/mmc/index.html"  target="_blank">Multimedia Computing Group (MMC)</a>, <a href="http://www.nlpr.ia.ac.cn/cn/index.html"  target="_blank">National Laboratory of Pattern Recognition</a>, 
<a href="http://www.ia.cas.cn/"  target="_blank">Institute of Automation, Chinese Academy of Sciences</a>, advised by <a href="http://nlpr-web.ia.ac.cn/mmc/homepage/csxu.html"  target="_blank">Prof. Changsheng Xu</a> (IEEE Fellow, IAPR Fellow, NSFC杰青). He was a research visitor in the China-Singapore Institute of Digital Media (CSIDM) and Department of Computing of The Hong Kong Polytechnic University respectively. 
Also, He was engaged in postdoctoral research in UESTC working with <a href="http://www.lxduan.info/"  target="_blank">Prof. Lixin Duan</a>. 

<br>
<br>

His research interests include machine learning and reasoning, question answering, and multimodal artificial intelligence.

<br>
<br>

</div>

<!*************************************************>


<!*************************** News*************************>
<div class="rightBox">
<p id="NEWS">
<font size="4"><b>NEWS</b></font> &nbsp; 
</p>

<ul>

<li> 
<font size="2" color="red">[<I>June 01, 2020</I>] </italic> </font>  Paper “Z. Yuan, S. Sun, L. Duan, C. Li, X. Wu, and C. Xu. "Adversarial Multimodal Network for Movie Question Answering" is accepted by IEEE Transactions on Multimedia.
</li>


</ul>
<br>

</div>



<!****************************** TEACHING******************************>
<div class="rightBox">
<p id="TEACHING">
<font size="4"><b>TEACHING</b></font> &nbsp; 
</p>

<ul>

<li>Digital Image Processing, B1250, 2020--2021(1), Text Book: <a href="https://www.amazon.com/Digital-Image-Processing-Rafael-Gonzalez/dp/0133356728/ref=sr_1_1?keywords=digital+image+processing&qid=1548059657&sr=8-1" target="_blank">Digital Image Processing (4th Edition), Rafael C. Gonzalez</a>.
</li>

<li>Digital Image Processing, B0445, 2019--2020(2), Text Book: <a href="https://www.amazon.com/Digital-Image-Processing-Rafael-Gonzalez/dp/0133356728/ref=sr_1_1?keywords=digital+image+processing&qid=1548059657&sr=8-1" target="_blank">Digital Image Processing (4th Edition), Rafael C. Gonzalez</a>.
</li>

<li>Digital Image Processing, B1932, 2019--2020(1), Text Book: <a href="https://www.amazon.com/Digital-Image-Processing-Rafael-Gonzalez/dp/0133356728/ref=sr_1_1?keywords=digital+image+processing&qid=1548059657&sr=8-1" target="_blank">Digital Image Processing (4th Edition), Rafael C. Gonzalez</a>.
</li>

<li>Digital Image Processing, B0332, 2018--2019(2).
</li>

</ul>

<br>

</div>





<!**********************************RESEARCH*********************************************>
<div class="rightBox">
<p id="RESEARCH">
<font size="4"><b>RECENT RESEARCH</b></font> &nbsp; <a href="#top">Go Top</a>
<br></p>




<!*******************---------------table-------------*************>
<table cellpadding="0" cellspacing="0" width="95%">
<tbody>

<!*******************---------Textbook Question Answering---------*************>
<tr bgcolor="#CCE5FF">
<th colspan="2" align="left">
<font color="black"><b>Multi-Modal Machine Comprehension (M3C)</b>

&nbsp;

<!--
[<a href="https://zqyuan.github.io/homepage/">Tutorial</a>]
-->

</font>
</th>
</tr>
<tr>
<td width="300" align="center"><img src="https://zqyuan.github.io/homepage/tqa.png" width="220">
</td>
<td>

Multi-Modal Machine Comprehension (M3C) task builds on the popular Visual Question Answering (VQA) and Machine Comprehension (MC) paradigms by framing question answering as a machine comprehension task, where the context needed to answer questions is provided and composed of both textual languages and visual images. 
We are engaged in developing advanced algorithms for multimodal learning and reasoning, especially for textbook question answering, diagram parsing, multi-step reasoning, neural-symbolic learning, etc.


<br>
<br>

<b>Datasets and codes: </b>
[<a href="https://allenai.org/data/tqa" target="_blank">Dataset</a>]&nbsp;
[<a href="https://zqyuan.github.io/homepage/">Code</a>]
</td>
</tr>
<!*******************---------Textbook Question Answering end---------*************>


<!*******************---------Stable prediction---------*************>
<tr bgcolor="#CCE5FF">
<th colspan="2" align="left">
<font color="black"><b>Stable Machine Learning</b>
&nbsp;

<!--
[<a href="https://zqyuan.github.io/homepage/">Tutorial</a>]
-->

</font>
</th>
</tr>
<tr>
<td width="300" align="center"><img src="https://zqyuan.github.io/homepage/stable-learning.png" width="220">
</td>
<td>

Conventional predictive models in machine learning are based on the I.I.D. hypothesis between training and testing data. However, such hypothesis is fragile in real world, and the model minimizing empirical errors on training data does not perform well on testing data, which makes the prediction unstable.
We made efforts to design advanced models to learn invariant representations for stable prediction by combining contrastive learning, causal inference, meta-learning, etc.



<br>
<br>

<b>Datasets and codes: </b>
[<a href="http://nico.thumedialab.com/" target="_blank">Dataset</a>]&nbsp;
[<a href="https://zqyuan.github.io/homepage/">Code</a>]
</td>
</tr>
<!**---------Stable prediction end--------------*>


<!*****-------------------------Multimodal Dialogue------------------------****>
<tr bgcolor="#CCE5FF">
<th colspan="2" align="left">
<font color="black"><b>Multimodal Conversation</b>
&nbsp;

<!--
[<a href="https://zqyuan.github.io/homepage/">Tutorial</a>]
-->

</font>
</th>
</tr>
<tr>
<td width="300" align="center"><img src="https://zqyuan.github.io/homepage/vis-dial.png" width="180">
</td>
<td>

Human-computer dialogue is one of the important research directions of natural language understanding and generation. 
  However, the current human-machine dialogue system is limited to the interactive form of speech or natural language. 
  In recent years, with the popularization and application of voice assistants, virtual digital people, and intelligent service robots, 
  the multimodal conversation with both "visual and listening" plays a more and more important role in the fields of retail, 
  customer service, media, education, tourism, etc. The multimodal (text + image) interaction makes the information distribution 
  more efficient and the interaction mode more natural. 
We are engaged in developing advanced algorithms for multimodal human-computer interaction system to understand the user's 
  intentions in a detailed way and give accurate and fast responses.


<br>
<br>

<b>Datasets and codes: </b>
[<a href="https://amritasaha1812.github.io/MMD/">Dataset</a>]&nbsp;
[<a href="https://zqyuan.github.io/homepage/">Code</a>]

</td>
</tr>

<!*****-------------------------Multimodal Dialogue end------------------------****>





<!*****-----------------****>

</tbody>
</table>

<br>
<br>

</div>





<!**********************SELECTED PUBLICATIONS**************************>

<div class="rightBox">
<p id="PUBLICATIONS">
<font size="4"><b>SELECTED PUBLICATIONS</b></font>  &nbsp; <a href="#top">Go Top</a>
</p>


<p>
<font size="3">
<b>Journal Papers</b></font>
</p>

<ul>

<li>
  <strong>Zhaoquan Yuan</strong>, Siyuan Sun, Lixin Duan<sup>*</sup>, Changsheng Li<sup>*</sup>, Xiao Wu and Changsheng Xu, "Adversarial Multimodal Network for Movie Story Question Answering," 
  in IEEE Transactions on Multimedia (<strong>TMM, JCR-1区</strong>), doi: 10.1109/TMM.2020.3002667.
</li>
  
<li>Jun-Yan He, Xiao Wu, Zhi-Qi Cheng, <strong>Zhaoquan Yuan</strong> and Yu-Gang Jiang. 
DB-LSTM: Densely-Connected Bi-directional LSTM for Human Action Recognition. in Neurocomputing (TMM, JCR-2区), 2020.
</li>
  
<li>
  <strong>Zhaoquan Yuan</strong>, Changsheng Xu<sup>*</sup>, Jitao Sang, Shuicheng Yan and M. Shamim Hossain, "Learning Feature Hierarchies: A Layer-Wise Tag-Embedded Approach," 
  in IEEE Transactions on Multimedia (<strong>TMM, JCR-1区</strong>), vol. 17, no. 6, pp. 816-827, June 2015, doi: 10.1109/TMM.2015.2417777.
</li>
  
<li>
<strong>Zhaoquan Yuan</strong>, Jitao Sang, Changsheng Xu<sup>*</sup> and Yan Liu, "A Unified Framework of Latent Feature Learning in Social Media," 
  in IEEE Transactions on Multimedia (<strong>TMM, JCR-1区</strong>), vol. 16, no. 6, pp. 1624-1635, Oct. 2014, doi: 10.1109/TMM.2014.2322338.
</li>

</ul>



<p>
<font size="3">
<b> Conference Papers</b></font>
</p>

<ul>

<li>Xiao Peng, <strong>Zhaoquan Yuan<sup>*</sup></strong>,  Huan Shao, Xiao Wu and Changsheng Xu,
"Meta-Learning Causal Convolutional Network for Stable Prediction,"
submitted to the 35th AAAI Conference on Artificial Intelligence (AAAI-21).
</li>

<li>Huan Shao, <strong>Zhaoquan Yuan<sup>*</sup></strong>, Xiao Peng and Xiao Wu,
"Contrastive learning in frequency domain for Non-I.I.D. image classification,"
submitted to the International MultiMedia Modeling Conference (MMM 2021).
</li>
  
<li><strong>Zhaoquan Yuan</strong>, Jitao Sang, Yan Liu and Changsheng Xu<sup>*</sup>,
"Latent feature learning in social media network,"
In Proceedings of the 21st ACM international conference on Multimedia (<strong>ACM MM, CCF-A</strong>), pp. 253-262. ACM, 2013.
</li>

<li><strong>Zhaoquan Yuan</strong>, Jitao Sang and Changsheng Xu<sup>*</sup>, 
"Tag-aware image classification via nested deep belief nets,"
In 2013 IEEE International Conference on Multimedia and Expo (ICME, CCF-B), pp. 1-6. IEEE, 2013.
</li>

</ul>

<br>
</div>




<!**************** Grant and Funds****************>

<div class="rightBox">
<p id="FUNDS">
<font size="4"><b>GRANT AND FUNDS</b> </font> &nbsp; <a href="#top">Go Top</a>
</p>

<ul>
<li>Machine Reasoning Research Towards Multimodal Question Answering, 2012.01 -- 2022.12, Sichuan Science and Technology Program, 2020YJ0037.</li>

<li>Machine Learning and Reasoning in Video Question Answering, 2019.01 -- 2021.12, the National Natural Science Foundation of China, 61802053.</li>

<li>Semantic Understanding toward Video Question Answering, 2019.01 -- 2020.12, the Fundamental Research Funds for the Central Universities, 2682019CX62.</li>
</ul>
<br>
</div>



<!**************** Services ****************>

<div class="rightBox">
<p id="FUNDS">
<font size="4"><b>SERVICES</b> </font> &nbsp; <a href="#top">Go Top</a>
</p>

<p>
<font size="3">
<b>Program Committee Members</b></font>
</p>

<ul>

<li>ACM International Conference on Multimedia 2020, 2019.</li>
<li>International Conference on Multimedia Modeling 2021, 2020, 2019.</li>
<li>ACM International Conference on Multimedia in Asia 2019.</li>
<li>ACM International Conference on Multimedia Retrieval 2019.</li>
<li>IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2019.</li>
<li>IEEE International Conference on Multimedia and Expo 2019. </li>

</ul>


<p>
<font size="3">
<b>Journal Reviewer</b></font>
</p>

<ul>


<li>IEEE Transactions on Multimedia.</li>
<li>Multimedia Tools and Applications.</li>

<br>
</div>





<!**************** students****************>

<div class="rightBox">
<p id="STUDENTS">
<font size="4"><b>STUDENTS</b> </font> &nbsp; <a href="#top">Go Top</a>
</p>

<p>
<font size="3">
<b>Students Collaborated With Me</b></font>
</p>

<ul>

<li>Huan Shao
</li>

<li>Xiao Peng
</li>

<li>Haitao Fang
</li>

<li>Zhou Du
</li>


</ul>

<br>
</div>





<!*********************** RESOURCE**************************>
<div class="rightBox">
<p id="RESOURCE">
<font size="4"><b>RESOURCE</b></font>  &nbsp; <a href="#top">Go Top</a>
</p>

<p>
<font size="3">
<b>Machine learning / NLP</b></font>
</p>

<ul>
<li><a href="https://github.com/changwookjun/nlp-paper"  target="_blank">NLP Paper List</a>, 
  <a href="https://github.com/sebastianruder/NLP-progress"  target="_blank">NLP-progress</a>,
  <a href="https://github.com/yizhen20133868/NLP-Conferences-Code"  target="_blank">NLP-Conf-Code</a>.
</li>
  
<li><a href="https://www.dgl.ai/"  target="_blank">Deep Graph Libray</a>
</li>

<li>DRL: <a href="https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"  target="_blank">DRL_Pytorch 1</a>,
  <a href="https://github.com/sweetice/Deep-reinforcement-learning-with-pytorch"  target="_blank">DRL_Pytorch 2</a>
</ul>

  
  

<!*****---------------- Others---------------- ***>

<p>
<font size="3">
<b>Others</b></font>
</p>

<ul>
<li><a href="http://www.zhuanzhi.ai/topic/2001826911580295" target="_blank">专知知识分享平台</a>
</li>

<li><a href="https://github.com/bharathgs/Awesome-pytorch-list"  target="_blank"> Awesome-Pytorch-list</a>
</li>

</ul>
<br>
</div>




<!*************************招生信息************************>
<div class="rightBox">
<p id="recruit">
<font size="4"><b>招生信息</b></font>  &nbsp; <a href="#top">Go Top</a>
</p>

袁召全博士本科毕业于中国科学技术大学计算机科学与技术学院，博士毕业于中国科学院自动化研究所 (模式识别国家重点实验室)。曾先后在中国-新加坡数字媒体研究院、香港理工大学计算机科学系交流访问。
  近年来在ACM Multimedia, IEEE Transactions on Multimedia等国际学术会议及期刊发表论文多篇，拥有国家专利2项。任多届MM, ICMR, ICME, MMM, MM Asian等国际会议程序委员会委员，
  任IEEE Transactions on Multimedia, Multimedia Tools and Applications等国际期刊审稿人。研究方向主要包括智能问答与对话、机器学习与推理、多模态语义理解等。
  目前主持国家自然科学基金青年项目1项，四川省科技计划项目1项，中央高校基本科研业务费项目1项。欢迎有上进心与科研热情、数学与编程基础扎实的同学加入科研团队。

<br>
<br>
有意向的同学请邮件咨询: zqyuan0@gmail.com。
<br>
<br>

</div>
<!*************************************************>




<!*****************************updated date***********************************>
<div class="rightBox">

Last updated date: June 01, 2020.
<br>
<br>

<!*********************************Visit tracker**********************************>





</script>

</body>

</html>
